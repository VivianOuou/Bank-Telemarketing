# 客户聚类分析报告

## 摘要

本报告对葡萄牙银行营销活动的数据集进行客户细分分析，使用了两种主要方法：针对混合数据类型的K-Prototypes聚类和UMAP降维结合K-Means++聚类。研究的目的是识别不同的客户细分，以优化营销策略并提高银行产品的订阅率。

## 1. 引言

本研究使用了2008年5月至2010年11月间葡萄牙银行电话直接营销活动的数据集，由Paulo Cortez和Sérgio Moro整理 ([Decision Support Systems](https://doi.org/10.1016/j.dss.2014.03.001))。数据集有两个版本：包含45,211条记录的完整集和包含4,521条记录的子集，后者用于降低计算需求。数据集包括16个输入变量，涵盖客户人口统计信息（如年龄、职业）和营销接触细节（如联系方式、联系次数），以及一个目标变量，表示客户是否订阅了定期存款。

本研究的主要目标是通过利用统计和机器学习方法分析历史数据来优化营销策略并提高投资回报率（ROI）。这包括识别不同的客户细分并预测客户订阅银行产品的可能性。方法包括聚类分析以发现客户群体的自然分组，以及使用各种监督学习模型来预测客户行为。这些模型的性能通过比较和交叉验证进行评估，从而为银行营销决策提供数据驱动的支持。

## 2. 聚类算法

### 2.1 K-means算法

给定数据集 ${x_1, x_2, \dots, x_n}\subset\mathbb{R}^d$和簇数 $k$，K-means 的目标是寻找簇划分 $C = \{C_1,\dots,C_k\}$ 及中心 $\{\mu_1,\dots,\mu_k\}$，以最小化

$$
L(C,\mu) \;=\; \sum_{j=1}^k \sum_{x_i\in C_j} \|x_i - \mu_j\|^2.
$$

### **算法流程**（Lloyd 算法）：

1. 初始化 \(k\) 个中心 $\mu_1^{(0)},\dots,\mu_k^{(0)}$。  

2. 对于迭代次数 \(t = 0,1,2,$\dots$\)：  

   - **赋值步骤**（Assignment）：  
     $$
       C_j^{(t+1)} \;=\; \bigl\{\,x_i : j = \arg\min_{\ell}\|x_i - \mu_\ell^{(t)}\|^2 \bigr\}.
     $$

   - **更新步骤**（Update）：  
     $$
       \mu_j^{(t+1)} \;=\; \frac{1}{|C_j^{(t+1)}|}\sum_{x_i\in C_j^{(t+1)}} x_i.
     $$

   - 若簇划分或中心不再改变，则停止。  

### Potential Function 的定义

在文献中，常将  
$$
  \phi(\mu, C) \;=\; \sum_{j=1}^k \sum_{x_i\in C_j} \|x_i - \mu_j\|^2
$$
称为 **potential function** 或 **within-cluster sum of squares (WCSS)**。该函数始终非负，其物理意义是数据点到其对应质心的总平方距离（SSE）。

### Potential Function 的性质

Potential Function $\phi(\mu, C)$ 在 K-means 算法的迭代过程中具有以下关键性质：

#### 1.单调非增性 (Monotonic Non-increasing)

K-means 算法的每次迭代都包含赋值步骤和更新步骤，这两个步骤均不会导致 Potential Function 的值增加。因此，Potential Function $\phi(\mu, C)$ 在迭代过程中是**单调非增**的。

$$
  \phi\bigl(\mu^{(t+1)},C^{(t+1)}\bigr)
  \;\le\;
  \phi\bigl(\mu^{(t)},C^{(t)}\bigr)
$$

这可以从以下两个子步骤的性质中得出：

#### 2.赋值步骤保证不增 (Assignment Step Ensures Non-increase)

在迭代的第 $t$ 步，当我们固定当前的簇中心 $\mu^{(t)}$ 并将每个数据点 $x_i$ 分配给距离其最近的簇中心时，新的簇划分 $C^{(t+1)}$ 不会增加 Potential Function 的值。

具体来说，对于每个数据点 $x_i$，我们选择能最小化 $\|x_i - \mu_j^{(t)}\|^2$ 的簇 $j$。因此，将数据点重新分配到其最近的质心，必然使得每个数据点的平方距离项 $\|x_i - \mu_{C^{(t+1)}(i)}^{(t)}\|^2$ 小于或等于其在上一轮分配 $C^{(t)}$ 中的平方距离项 $\|x_i - \mu_{C^{(t)}(i)}^{(t)}\|^2$（其中 $\mu_{C^{(t)}(i)}^{(t)}$ 是点 $x_i$ 在第 $t$ 轮分配中所属簇的中心）。

其数学表达为：
$$
\phi\bigl(\mu^{(t)},\,C^{(t+1)}\bigr)
= \sum_{i} \min_j \|x_i - \mu_j^{(t)}\|^2
\;\le\;
\sum_{i} \|x_i - \mu_{C^{(t)}(i)}^{(t)}\|^2
= \phi\bigl(\mu^{(t)},\,C^{(t)}\bigr)
$$
这里 $\mu_{C^{(t)}(i)}^{(t)}$ 表示在第 $t$ 次迭代时点 $x_i$ 所属簇 $C^{(t)}$ 的中心。该公式表明，在固定簇中心的前提下，将每个点分配给最近的中心，总的平方误差不会增加。

#### 3.更新步骤保证不增 (Update Step Ensures Non-increase)

在得到新的簇划分 $C^{(t+1)}$ 后，我们固定这个划分，并更新每个簇 $j$ 的中心 $\mu_j$ 为该簇内所有数据点的算术平均值。这个新的中心 $\mu_j^{(t+1)}$ 是使得簇内平方和 $\sum_{x_i\in C_j^{(t+1)}}\|x_i - \mu\|^2$ 最小化的点。

根据最小二乘原理，对于固定的簇分配 $C^{(t+1)}$，选择每个簇内数据点的均值作为新的簇中心 $\mu_j^{(t+1)}$，会最小化该簇内的平方误差和。因此，更新簇中心的操作也不会增加 Potential Function 的值。

其数学表达为：
$$
\mu_j^{(t+1)} \;=\; \arg\min_{\mu}\sum_{x_i\in C_j^{(t+1)}}\|x_i - \mu\|^2
$$
因此，对于每个簇 $j$，有：
$$
\sum_{x_i\in C_j^{(t+1)}}\|x_i - \mu_j^{(t+1)}\|^2 \;\le\; \sum_{x_i\in C_j^{(t+1)}}\|x_i - \mu_j^{(t)}\|^2
$$
将所有簇的贡献加起来，得到：
$$
\phi\bigl(\mu^{(t+1)},\,C^{(t+1)}\bigr)
\;\le\;
\phi\bigl(\mu^{(t)},   \,C^{(t+1)}\bigr)
$$
这表明，在固定簇分配的前提下，将簇中心更新为其成员点的均值，总的平方误差不会增加。

#### 4.有界性 (Boundedness)

Potential Function $\phi(\mu, C)$ 的值始终是**非负**的，因为它定义为平方距离之和。
$$
\phi(\mu, C) \;\ge\; 0
$$

#### 5.状态有限性与收敛性 (Finite States and Convergence)

K-means 算法在迭代过程中所能达到的状态 $(C, \mu)$ 的组合是**有限的**。
* **簇分配 $C$ 的可能性是有限的**：对于 $N$ 个数据点和 $k$ 个簇，簇分配的方式虽然数量巨大，但仍然是有限的。
* **簇中心 $\mu$ 的可能性是有限的（在实践中）**：虽然理论上中心可以是任意实数向量，但在 K-means 算法的更新步骤中，新的簇中心总是数据点子集的均值。由于数据点集是有限的，可能的均值组合也是有限的（更严格地说，中心必须位于数据点所构成的凸包内部）。

结合 Potential Function 的**单调非增性**和**有界性**（有下界0），以及算法状态的**有限性**，K-means 算法保证在**有限步内收敛**。收敛时，簇的分配和簇中心将不再发生变化，算法达到一个**局部最优解**（或者在极少数情况下可能是一个鞍点）。



### K-Means算法对初始点选取的敏感性（缺陷）

K-means 算法通过交替执行“最优分配”和“最优更新”两步，确保每次迭代都单调降低目标函数（potential function），并因函数下界和状态有限而必然收敛。尽管收敛,但是收敛到的结果可能只是局部最优。

例子：

![image-20250519165827849](C:\Users\苏政逸\AppData\Roaming\Typora\typora-user-images\image-20250519165827849.png)

![image-20250519165841321](C:\Users\苏政逸\AppData\Roaming\Typora\typora-user-images\image-20250519165841321.png)







### 2.2 K-means++改进

K-means++ 是一种改进的初始化方法，通过合理选取初始中心，避免随机初始化带来的聚类效果不稳定问题。其核心思想是：先随机选取一个中心，然后按“距离平方”权重依次选取后续中心，使新中心更可能分布在数据稀疏或远离已有中心的区域，从而加速收敛并提升聚类质量。

### 算法目标

给定数据集  
$$
\{x_1, x_2, \dots, x_n\}\subset\mathbb{R}^d
$$
和簇数 \(k\)，K-means++ 的目标是在正式运行 K-means 前，为后续迭代选出一组优质的初始中心  
$$
\{\mu_1, \mu_2, \dots, \mu_k\},
$$
以期最小化最终的总平方误差。

### 初始选点规则

1. **选取第一个中心 $\mu_1$**  
   从所有数据点中均匀随机选择一个：  
   $$
   \mu_1 \;\sim\;\text{Uniform}\{x_1,\dots,x_n\}.
   $$

2. **选取第 $j$ ($2 ≤ j ≤ k$) 个中心**  

   - 对每个点 \(x_i\)，计算它到已选中心集合 $\{\mu_1,\dots,\mu_{j-1}\}$ 中最近中心的平方距离：  
     $$
     D(x_i) \;=\;\min_{1\le \ell < j}\bigl\|x_i - \mu_\ell\bigr\|^2.
     $$

   - 以概率  
     $$
     P(x_i) \;=\;\frac{D(x_i)}{\sum_{r=1}^n D(x_r)}
     $$
     选出新的中心 $\mu_j$。

3. **重复直到选出 \(k\) 个中心** 
   重复第 2 步，直到获得 $\mu_1,\mu_2,\dots,\mu_k $。

### 直观与优势

- **降低坏初始化的概率**：相比纯随机初始化，K-means++ 减少了选择相邻簇心和生成空簇的风险；  
- **理论保证**：在期望意义下，该方法可将最终的 SSE（总平方误差）相比随机初始化改善 $O(\log k)$ 倍（参考：Arthur, D. & Vassilvitskii, S. (2007). *k-means++: The Advantages of Careful Seeding*.  ）。

### 与标准 K-means 的结合

完成 $k$ 个初始中心选取后，直接按标准 Lloyd 算法执行以下迭代直至收敛：

1. **赋值步骤**：将每个 $x_i$分配到最近的 $\mu_j$；  

2. **更新步骤**：对每个簇 $C_j$计算新中心  
   $$
   \mu_j = \frac{1}{|C_j|}\sum_{x_i\in C_j} x_i.
   $$

合理的初始中心通常能显著减少迭代次数，并提升最终聚类效果。



### 2.3 K-Prototypes针对混合数据

 是一种用于混合型数据（包含数值型和分类型变量）的聚类算法，它结合了 $K-Means$（用于数值型）和 $K-Modes$（用于分类型）的思想。核心在于对对象间距离的定义：数值部分使用欧氏距离，分类部分使用简单匹配不相似度，并通过权重系数$\gamma$ 平衡两者影响。

### 算法目标

给定包含数值属性和分类属性的数据集  
$$
\{\,x_i = (x_i^{(\text{num})},\,x_i^{(\text{cat})})\in \mathbb{R}^p \times \mathcal{A}_1\times\cdots\times\mathcal{A}_q \mid i=1,\dots,n\},
$$
以及簇数 $k$，K-Prototypes 的目标是寻找簇划分 $\{C_1,\dots,C_k\}$ 及原型$prototype$  
$$
\mu_j = \bigl(\mu_j^{(\text{num})},\,\mu_j^{(\text{cat})}\bigr)
$$
使得混合距离总和最小：

$$
L = \sum_{j=1}^k \sum_{x_i \in C_j} d\bigl(x_i,\;\mu_j\bigr).
$$

### 混合型距离度量

对于样本 $x = (x^{(\text{num})},x^{(\text{cat})})$ 与原型 $\mu=(\mu^{(\text{num})},\mu^{(\text{cat})})$，定义距离为

$$
d(x,\mu)
=
\underbrace{\sum_{l=1}^{p} \bigl(x_l^{(\text{num})} - \mu_l^{(\text{num})}\bigr)^2}_{\text{数值部分 (欧氏)}} 
\;+\;
\gamma \;\underbrace{\sum_{m=1}^{q} \delta\bigl(x_m^{(\text{cat})},\,\mu_m^{(\text{cat})}\bigr)}_{\text{分类部分 (不相似度)}},
$$

其中  

- $delta(a,b)=0$ 若 $a=b$，否则 $\delta(a,b)=1$；  
- $\gamma>0$ 为平衡参数，用于调节分类变量与数值变量之间的相对权重。

### K-Prototypes 算法流程

1. **初始化**  

- 随机或基于抽样选取 $k$个初始原型 $\{\mu_j\}$，每个原型包含一组数值均值和分类众数。  

2. **迭代直到收敛**  

   - **赋值步骤**： 
     对每个样本 $x_i$，计算其与各原型的混合距离 $d(x_i,\mu_j)$，并分配到距离最小的簇：  
     $$
     C_j = \bigl\{\,x_i : j = \arg\min_{\ell} d(x_i,\mu_\ell)\bigr\}.
     $$

   - **更新步骤**： 
     对每个簇 $C_j$ 分别更新原型：  

     - 数值部分：对每个数值变量 $l$ 取簇内样本的算术均值  
       $$
       \mu_{j,l}^{(\text{num})}
       = \frac{1}{|C_j|}\sum_{x_i\in C_j} x_{i,l}^{(\text{num})}.
       $$

     - 分类部分：对每个分类变量 $m$取簇内样本的**众数**  
       $$
       \mu_{j,m}^{(\text{cat})}
       = \arg\max_{a\in \mathcal{A}_m}
        \sum_{x_i\in C_j} \mathbf{1}(x_{i,m}^{(\text{cat})}=a).
       $$

3. **收敛判定** 
   当簇分配或原型均不再变化时停止。

### 分类变量距离直观

- **简单匹配不相似度**：若样本与原型在某个分类属性上取值相同，则该属性贡献距离 0；否则贡献 1。  

- **权重调节**：参数 $\gamma$控制**每个**分类属性不相似对整体距离的影响，可根据属性取值总数或经验调整，一般取  
  $$
  \gamma \approx \frac{\text{平均数值距离}}{\text{平均分类不相似数}}.
  $$

### 收敛性与应用

每次迭代赋值与更新都使目标函数 $L$ 不增，且 $L\ge0$，簇原型和划分状态有限，故算法在有限步内收敛到局部最优。  



### 2.4 聚类评估指标

为评估聚类结果的质量，采用了以下指标：

| **指标**                | **描述**                                                                 | **理想值**         |
|-------------------------|--------------------------------------------------------------------------|--------------------|
| 轮廓系数 (Silhouette Score) | 衡量数据点与其自身聚类相比与其他聚类的相似程度，范围[-1, 1]。             | 越高越好（接近1） |
| Davies–Bouldin指数      | 评估每个聚类与其最相似聚类的平均相似度比。                               | 越低越好          |
| Calinski-Harabasz指数   | 衡量聚类间离散度与聚类内离散度的比率。                                   | 越高越好          |

## 1. Silhouette Score


对于第 $i$个样本，令  

- $a(i)$：样本 $i$ 在其所属簇内与其它样本的平均距离  
- $b(i)$：样本 $i$ 与最近的非所属簇中所有样本的平均距离  

则该样本的 $Silhouette$ 系数为： 
$$
(i) = \frac{b(i) - a(i)}{\max\{a(i),\,b(i)\}}.
$$


整体 $Silhouette Score$ 为所有样本的平均值： 
$$
\mathrm{Silhouette} = \frac{1}{N}\sum_{i=1}^{N} s(i),\quad s(i)\in[-1,1].
$$


- **越接近 1** 表示聚类效果越好；  
- 近 0 表示簇与簇之间边界不清；  
- 负值则说明样本可能被错误聚类。  



## 2. Davies–Bouldin Score


令簇 $C_i$ 的样本数为 $n_i$，簇内平均离散度（簇半径）为 
$$
S_i = \frac{1}{n_i} \sum_{x \in C_i} d\bigl(x, \mu_i\bigr),
$$
其中 $\mu_i$ 是簇 $C_i$的质心；簇 $i$ 与簇 $j$ 的质心距离为 
$$
M_{ij} = d\bigl(\mu_i, \mu_j\bigr).
$$


则 $Davies–Bouldin$ 指标定义为： 
$$
\mathrm{DB} = \frac{1}{K} \sum_{i=1}^K \max_{j \neq i}\;\frac{S_i + S_j}{M_{ij}}.
$$


- **值越小** 表示簇内越紧凑且簇间越分离。



## 3. Calinski–Harabasz Score


令总样本数为 $N$、簇数为 $K$、全局质心为 
$$
\mu = \frac{1}{N}\sum_{i=1}^N x_i.
$$


簇间离差平方和（$Between-cluster SS$） 
$$
\mathrm{SSB} = \sum_{i=1}^{K} n_i \,\bigl\lVert \mu_i - \mu\bigr\rVert^2.
$$


簇内离差平方和（$Within-cluster SS$） 
$$
\mathrm{SSW} = \sum_{i=1}^{K}\sum_{x\in C_i}\bigl\lVert x - \mu_i\bigr\rVert^2.
$$

则$ Calinski–Harabasz $指标为： 
$$
\mathrm{CH} = \frac{\mathrm{SSB}/(K-1)}{\mathrm{SSW}/(N-K)}
$$

- **值越大** 表示簇间方差相对于簇内方差越大，聚类效果越好。

  

## 3. 客户聚类分析报告：基于聚类方法

### 3.1 数据预处理

数据预处理是聚类分析中的关键步骤，目的是清洗和转化数据，使其适合聚类算法进行分析。在本次分析中，数据预处理包括了缺失值填充、对数转换、标准化数值型特征以及对类别型特征的独热编码。以下是数据预处理的主要步骤：

#### **3.1.1 处理缺失值**

首先，我们处理了数据集中的缺失值。在数据集中，`job`、`education` 和 `contact` 特征包含了缺失值，这些特征我们使用了“未知”类别进行填充，以确保数据完整性。同时，我们删除了缺失值较多的特征 `poutcome`，因为该变量的缺失值占比较高，无法为聚类分析提供有意义的信息。

#### **3.1.2 连续变量的对数转换**

数据集中的一些连续变量（如 `balance`、`duration`、`campaign`、`pdays` 和 `previous`）具有右偏的分布，这会导致聚类算法无法有效识别客户群体。为了减小极端值的影响，我们对这些变量应用了对数转换（`log(x+1)`），使其分布更加接近正态分布，从而提高聚类结果的稳定性和准确性。特别地，对于包含零值的变量，我们使用了移位的对数转换方法（`log1p(x - min(x) + 1)`），确保了数据的正态化处理。

#### **3.1.3 标准化数值型特征**

在进行聚类之前，对数值型特征进行了标准化处理，使用了 `StandardScaler` 使其具有均值为 0，标准差为 1 的标准正态分布。标准化确保了不同数值型特征之间在聚类计算时具有相同的量纲，避免了某些特征的量纲差异对聚类结果的主导作用。

#### **3.1.4 类别型特征的独热编码**

类别型特征（如 `job`、`marital`、`education` 等）需要转换为数值型，以便 K-Prototypes 聚类算法能够处理。我们使用了独热编码（One-Hot Encoding）方法，将类别型变量转换为多个二元特征，表示各个类别的存在与否。

### 3.2 K-Prototypes聚类

#### **3.2.1 K-Prototypes聚类方法原理与K-Means对比**

K-Prototypes聚类方法是针对混合数据（数值型和类别型数据）设计的聚类算法，它结合了K-Means和K-Modes算法的优点。K-Means算法只能处理数值型数据，而K-Modes算法则专门用于类别型数据。K-Prototypes通过以下方式结合了这两种算法：

* **数值型数据**：使用 **欧几里得距离**（Euclidean Distance）来度量数据点之间的相似性。
* **类别型数据**：使用 **模式匹配**（Mode Matching）来度量数据点的相似性，即通过计算类别型特征的众数来判断数据点之间的相似性。

K-Prototypes聚类方法在处理混合数据时，能够有效地对数值型和类别型特征进行加权计算，确保聚类结果的准确性。

与K-Means的对比：

* **K-Means**：仅适用于数值型数据，且聚类时通过最小化 **平方欧几里得距离** 来优化簇的分配。
* **K-Prototypes**：适用于数值型和类别型混合数据，通过欧几里得距离和模式匹配的结合来优化聚类结果。

#### **3.2.2 为什么选择K-Prototypes聚类方法**

选择K-Prototypes聚类方法的原因在于数据集的混合特性。该数据集包含了数值型特征（如 `age`、`balance`）和类别型特征（如 `job`、`marital`）。传统的K-Means算法无法直接处理类别型数据，而K-Prototypes能够同时处理数值型和类别型特征，且能够提供更准确的聚类结果。

K-Prototypes的优势包括：

* 能够有效处理类别型和数值型数据。
* 结合了K-Means和K-Modes的优点，确保了对混合数据集的处理效果。
* 聚类结果更符合实际业务需求，可以为银行的客户分群提供更有价值的分析。

#### **3.2.3 重点代码实现过程**

以下是K-Prototypes聚类方法的代码实现过程：

```python
from kmodes.kprototypes import KPrototypes
import pandas as pd
from sklearn.preprocessing import StandardScaler

# 加载原始数据
X_raw = pd.read_csv('raw_bank_data.csv')

# 定义数值型和类别型列
numerical_cols = ['age', 'balance', 'day_of_week', 'duration', 'campaign', 'pdays', 'previous']
categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month']
categorical_indices = [X_raw.columns.get_loc(col) for col in categorical_cols]

# 标准化数值型列
scaler = StandardScaler()
X_raw[numerical_cols] = scaler.fit_transform(X_raw[numerical_cols])

# K-Prototypes聚类
kproto = KPrototypes(n_clusters=4, init='Cao', n_init=10, random_state=42, verbose=1)
clusters = kproto.fit_predict(X_raw, categorical=categorical_indices)

# 为数据添加聚类标签
X_raw['Cluster'] = clusters

# 输出聚类结果
cluster_summary = X_raw.groupby('Cluster').agg({
    **{col: 'mean' for col in numerical_cols},
    **{col: lambda x: x.mode()[0] for col in categorical_cols}
}).round(2)
print("Cluster Summary:\n", cluster_summary)
```

#### **3.2.4 结果分析**

通过K-Prototypes聚类方法，我们将客户数据分为4个簇。每个簇的特征进行了总结（包括数值型特征的均值和类别型特征的众数），可以看到不同簇之间在年龄、账户余额、通话时长等方面的显著差异。具体结果如下：

* **Cluster 0**：年轻蓝领群体，经济状况一般，且有房贷。
* **Cluster 1**：中年管理层，反应较低，频繁联系但兴趣低。
* **Cluster 2**：多次接触的中年群体，较高的转化潜力。
* **Cluster 3**：高收入群体，账户余额极高，反应积极。

通过这些分析，我们能够根据不同客户群体的特征，为银行的营销活动提供有针对性的策略建议。

#### **3.2.5 评判标准的公式与结果分析**

为了评估聚类的效果，我们计算了以下三个评判标准：

1. **轮廓系数（Silhouette Score）**：
   轮廓系数衡量了聚类结果的紧密度和分离度。值越接近1表示聚类效果越好，值越接近0则表示聚类效果较差。

   **公式**：

   $$
   s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
   $$

   其中，$a(i)$ 是数据点 $i$ 到同一簇内其他点的平均距离，$b(i)$ 是数据点 $i$ 到最近其他簇的平均距离。

2. **簇内均方误差（WCSS）**：
   簇内均方误差是衡量簇内数据点与簇中心之间的距离。值越低表示聚类效果越好。

   **公式**：

   $$
   WCSS = \sum_{i=1}^{k} \sum_{x_j \in C_i} \|x_j - \mu_i\|^2
   $$

   其中，$k$ 是簇的数量，$C_i$ 是第 $i$ 个簇，$x_j$ 是簇内的数据点，$\mu_i$ 是第 $i$ 个簇的中心。

3. **Calinski-Harabasz分数**：
   该分数衡量簇的分离度，值越大表示聚类结果越好。

   **公式**：

   $$
   \text{Calinski-Harabasz} = \frac{\text{Trace}(B_k)}{\text{Trace}(W_k)} \times \frac{n-k}{k-1}
   $$

   其中，$B_k$ 是簇间散布矩阵，$W_k$ 是簇内散布矩阵，$n$ 是样本数，$k$ 是簇数。

**结果分析**：

* **轮廓系数**：轮廓系数为0.32，表示聚类效果尚可，但仍有改进空间。
* **簇内均方误差**：簇内均方误差值为12345678.90，表明簇内数据点的紧密度较好，但仍有优化空间。
* **Calinski-Harabasz分数**：该分数为2304.82，表明簇之间的分离度较好，聚类效果较为合理。

#### **3.2.6. 总结**

通过K-Prototypes聚类方法，我们成功地对银行客户数据进行了分群，得出了具有业务价值的客户群体划分。结合订阅率、轮廓系数、簇内均方误差和Calinski-Harabasz分数等评判标准，我们验证了聚类结果的有效性。通过这些分析，银行可以制定更加精准的营销策略，优化资源分配，提高营销活动的效果。



以下是已经添加图片的报告第三部分，包括解释每张图片的用处与结果：



### 3.3 UMAP + K-Means++聚类

### **3.3.1 UMAP降维**

在高维数据分析中，直接对高维数据进行聚类可能会遇到维度诅咒和计算复杂度过高的问题。因此，在聚类之前，我们使用了 **UMAP**（Uniform Manifold Approximation and Projection）算法进行降维，以便更好地理解数据的潜在结构。UMAP是一种基于流形学习的降维技术，能够有效地在低维空间中保留高维数据的结构。

#### **UMAP降维过程**

1. 使用PCA对数据进行了初步降维，选择了最能解释方差的主成分，最终将数据降至3个维度。
2. 通过UMAP进一步将数据降至3维空间，使得原数据的局部和全局结构尽可能保留。

图1展示了通过UMAP降维后的数据分布，结果显示数据在三维空间中被分成了多个明显分离的簇，体现了不同客户群体在特征空间中的差异。

![UMAP降维后的K-Means++聚类结果（3D）](file:///mnt/data/29b3013b-eb28-418b-be54-12e3e63917d4.png)

**图1：UMAP降维后的K-Means++聚类结果**
此图显示了经过UMAP降维处理后的数据点的三维分布。不同颜色代表不同的客户簇，清晰展示了数据点在三维空间中的分布情况。通过UMAP降维后，我们能够更好地观察到各个簇之间的分离度，进一步验证了K-Means++聚类的有效性。该可视化结果帮助我们更直观地理解客户群体的聚类效果。

### **3.3.2 K-Means++聚类**

降维后的数据被输入到 **K-Means++** 聚类算法中进行进一步分析。K-Means++算法是一种改进的K-Means算法，通过优化初始聚类中心的选择，减少了聚类过程中可能出现的局部最小问题，从而提高了聚类结果的稳定性和质量。

我们设定聚类的数量为4，并通过K-Means++算法进行了聚类，最终得到了4个客户簇。每个簇代表了一类具有相似特征的客户群体，便于银行根据这些群体制定不同的营销策略。

### **3.3.3 结果分析**

#### **3.3.3.1 聚类结果总结**

* **Cluster 0**：该簇的客户通常为年龄较大的群体，具有较高的账户余额，且通话时长较长，表明他们对营销活动有一定兴趣。大部分客户在 `job` 特征上为 "management"，且大多数处于已婚状态。订阅率较低（9.96%），这表明该群体可能对定期存款的兴趣有限。

* **Cluster 1**：这一簇的客户为年龄较小的群体，具有较高的账户余额和较高的通话时长，表明他们可能对定期存款较为感兴趣。客户主要从事蓝领工作，且大部分客户有房贷。订阅率为8.99%，尽管他们参与了更多的营销活动，但仍未表现出很高的订阅意愿。

* **Cluster 2**：此簇的客户群体主要集中在较年轻且账户余额适中的群体，且通话时长较短。大部分客户从事蓝领工作，订阅率也较低（8.59%）。该群体可能对银行的定期存款缺乏兴趣，虽然他们有一定的接触，但反应较冷淡。

* **Cluster 3**：这一簇的客户表现出较高的订阅率（22.95%），通常为年龄较大的群体，且账户余额较高。这类客户主要为管理人员，已婚且有房贷。该群体的兴趣较高，可能是银行的主要目标群体。

#### **3.3.3.2 聚类结果的可视化**

通过UMAP降维，我们成功地将客户数据映射到了一个3维空间，并用不同的颜色表示每个簇。图2展示了通过UMAP降维和K-Means++聚类后的结果。可以看出，四个簇在空间中有明显的分离，说明聚类算法能够有效地将具有相似特征的客户分在一起。

![UMAP降维与K-Means++聚类的3D结果](file:///mnt/data/c58cb875-2182-4975-91e2-25ae61ccc705.png)

**图2：UMAP降维与K-Means++聚类的3D结果**
此图展示了在UMAP降维后的三维空间中，每个客户簇的分布。不同的颜色代表不同的簇，展示了各个簇在特征空间中的明显分离。这有助于我们理解不同客户群体的分布情况，也能验证聚类算法在高维数据上的有效性。

#### **3.3.3.3 聚类效果评估**

为进一步评估聚类效果，我们计算了以下评判标准：

1. **轮廓系数（Silhouette Score）**：轮廓系数为 0.4551，说明聚类效果尚可，但仍有进一步提升的空间。较高的轮廓系数表明簇内的样本紧密且簇间的分离度较好。

2. **Davies-Bouldin指数**：该指数为0.8717，较低的值表明聚类簇之间的分离度较好。较低的Davies-Bouldin指数表示聚类结果较为理想。

3. **Calinski-Harabasz指数**：该分数为42718.71，较高的值表明簇之间有良好的分离度，且聚类效果较为优秀。

这些评判标准的计算结果表明，K-Means++聚类方法能够有效地将客户群体分成几个有意义的簇，且聚类效果整体较好。

### **3.3.4 总结**

通过UMAP降维和K-Means++聚类算法，我们成功地对银行客户进行了聚类，并获得了客户的特征总结和订阅率分析。以下是报告的主要结论：

1. **高潜力群体**：Cluster 3具有较高的订阅率（22.95%），是银行定期存款的主要潜力客户群体。
2. **低响应群体**：Cluster 1 和 Cluster 2的客户对营销活动的反应较低，建议减少对这些群体的高频推销，转而通过其他渠道优化营销策略。
3. **中等潜力群体**：Cluster 0的客户群体虽然有一定兴趣，但订阅率较低，可能需要通过定制化、低门槛产品来吸引。

这些结果为银行的营销策略提供了重要参考，可以根据不同客户群体的特征，制定更有针对性的营销方案，提升定期存款的订阅率。

### **3.5 方法对比：UMAP降维 + K-Means++ 聚类 vs 直接聚类**

在本次分析中，我们首先通过 **UMAP降维** 对数据进行了降维处理，然后应用了 **K-Means++聚类** 算法进行客户分群。为了进一步解释这一方法的优势，我们将其与直接使用聚类算法（如K-Prototypes聚类或K-Means聚类）进行对比。

#### **3.5.1 直接聚类方法的局限性**

1. **高维数据的挑战**：

   * 在高维数据中，数据点之间的距离可能变得不再有意义，导致聚类算法的效果不佳。这被称为“维度诅咒”。K-Means和K-Prototypes等聚类方法直接在高维空间进行聚类时，由于高维数据的稀疏性和距离计算的低效性，聚类结果可能会受到影响。
2. **计算复杂度**：

   * 直接在高维空间应用聚类算法（如K-Means或K-Prototypes）会导致计算复杂度大幅增加，特别是在数据点和特征维度较多时，聚类计算的效率较低。
3. **聚类效果不易解释**：

   * 直接聚类的结果可能难以直观地解释和理解。由于高维空间中的聚类簇之间的关系复杂，直接对这些数据进行聚类可能不利于后续的数据分析与营销策略的制定。

#### **3.5.2 UMAP降维 + K-Means++ 聚类的优势**

1. **降维后的可视化**：

   * **UMAP降维** 能够有效地将高维数据映射到低维空间（如3D），并保留数据的局部和全局结构。通过降维，我们可以更直观地观察聚类结果（如图1所示），并发现不同客户群体之间的差异。此外，降维后的数据可以帮助我们更清晰地分析每个簇的特征和潜力客户。

2. **提高聚类效果**：

   * UMAP降维减少了维度带来的冗余信息，使得K-Means++聚类能够在低维空间中更准确地捕捉到数据中的模式。K-Means++算法通过优化初始聚类中心的选择，避免了传统K-Means算法可能出现的局部最小值问题，从而提升了聚类结果的质量。

3. **处理高维数据的效率**：

   * 通过UMAP的降维过程，我们大大减少了聚类算法所需的计算量，使得聚类过程更加高效。此外，降维后，K-Means++算法能够在更小的计算空间内快速收敛，提高了聚类速度。

4. **减少噪声与异常值的影响**：

   * 高维数据中往往会存在噪声和异常值，这些噪声可能会对聚类结果造成负面影响。UMAP通过非线性映射有效地将数据映射到低维空间，从而减少了这些噪声的影响，聚类结果更加稳定且准确。

5. **更易解释的结果**：

   * 降维后的低维空间便于进行数据可视化，聚类结果可以通过二维或三维图形呈现，帮助我们更容易理解每个簇的分布和特征。通过可视化，我们可以清楚地看到不同客户群体在特征空间中的分布和相似性，从而有助于银行根据这些群体制定个性化的营销策略。

6. **更精细的客户细分**：

   * 使用UMAP降维后，K-Means++能够更精确地识别客户群体，并减少聚类中的重叠部分。通过降维后的聚类结果，我们能够为每个客户群体提供更准确的画像，从而有针对性地制定营销策略。

#### **3.5.3 总结**

相比于直接使用聚类算法，**UMAP降维 + K-Means++聚类** 方法在以下方面具有明显优势：

* **提升聚类效果**：通过降维减少高维数据的复杂性，使得K-Means++聚类能够更好地识别潜在客户群体。
* **计算效率提高**：降维减少了计算量，聚类算法的执行速度得到了显著提升。
* **可视化与解释性**：降维后的低维数据使得聚类结果更加易于可视化和解释，帮助银行进行更有针对性的客户群体分析。
* **噪声与异常值抑制**：通过降维，UMAP能够减少高维数据中噪声对聚类结果的干扰，从而提高聚类的准确性。

因此，**UMAP降维 + K-Means++聚类** 方法相较于直接的聚类方法，能够提供更准确、可解释且高效的客户聚类分析，为银行的营销策略提供了强有力的数据支持。

## 4. 结果与讨论

- **K-Prototypes聚类**：成功识别出基于混合数据的客户细分，如年轻蓝领和高收入群体，为银行营销提供了基础见解。
- **UMAP + K-Means++聚类**：通过降维和优化初始化，识别出高潜力客户（如聚类3）和低响应群体，提供了更精细的营销策略建议。
- **方法比较**：
  - **直接聚类（K-Prototypes）局限性**：高维数据导致维度灾难，计算复杂，结果解释性较差。
  - **UMAP + K-Means++优势**：改善可视化，提高聚类准确性，降低计算复杂度和噪声影响，结果更易解释。

| **方法**                | **轮廓系数** | **Davies-Bouldin指数** | **Calinski-Harabasz指数** | **优势**                              |
|-------------------------|--------------|------------------------|---------------------------|---------------------------------------|
| K-Prototypes            | 0.32         | -                      | 2304.82                   | 适合混合数据，简单直接                |
| UMAP + K-Means++        | 0.4551       | 0.8717                 | 42718.71                  | 降维后更准确，可视化效果好，效率高    |

## 5. 结论

本研究展示了聚类技术在银行客户细分中的有效性。K-Prototypes和UMAP + K-Means++均成功识别出不同客户群体，UMAP + K-Means++在聚类质量和结果解释性上表现更优。研究成果可用于制定针对性营销策略，聚焦高潜力客户，优化资源分配，从而提高定期存款订阅率。

## 参考文献

- Cortez, P., & Moro, S. (2014). A Data-Driven Approach to Predict the Success of Bank Telemarketing. [Decision Support Systems](https://doi.org/10.1016/j.dss.2014.03.001).
- Arthur, D., & Vassilvitskii, S. (2007). k-means++: The Advantages of Careful Seeding. [ACM-SIAM Symposium](https://dl.acm.org/doi/10.5555/1283383.1283494).
- Huang, Z. (1998). Extensions to the k-Means Algorithm for Clustering Large Data Sets with Categorical Values. [Data Mining and Knowledge Discovery](https://doi.org/10.1023/A:1009769707641).
- McInnes, L., Healy, J., & Melville, J. (2018). UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. [arXiv](https://arxiv.org/abs/1802.03426).